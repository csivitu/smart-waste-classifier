{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diptesh2006/smart-waste-classifier/blob/main/Smart_WasteClassification_Forkthis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwpAk5eO_2L6"
      },
      "outputs": [],
      "source": [
        "import os, glob, shutil, zipfile, random, math\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rztUkOnyBD7A"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\" upload your dataset .zip file \")\n",
        "uploaded = files.upload()\n",
        "\n",
        "zip_path = list(uploaded.keys())[0]\n",
        "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "    zf.extractall(\"/content/trashnet\")\n",
        "\n",
        "print(\"Dataset at /content/trashnet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXb6cO9KEOQi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "BASE = \"/content/trashnet/dataset-resized\"\n",
        "\n",
        "\n",
        "paths = sum([glob.glob(os.path.join(BASE, c, \"*.jpg\"))\n",
        "             for c in os.listdir(BASE) if os.path.isdir(os.path.join(BASE, c))], [])\n",
        "labels_str = [os.path.basename(os.path.dirname(p)) for p in paths]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW5Jvb2gKIfF"
      },
      "outputs": [],
      "source": [
        "X_tr, X_val, y_tr_str, y_val_str = train_test_split(\n",
        "    paths, labels_str, test_size=0.2, stratify=labels_str, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI3FPpP29iqf"
      },
      "outputs": [],
      "source": [
        "class_names   = set(labels_str)\n",
        "class_to_idx  = {c:i for i,c in enumerate(class_names)}\n",
        "idx_to_class  = {i:c for c,i in class_to_idx.items()}\n",
        "y_tr  = np.array([class_to_idx[s] for s in y_tr_str],  dtype=np.int32)\n",
        "y_val = np.array([class_to_idx[s] for s in y_val_str], dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAEjcsHT9z_m"
      },
      "outputs": [],
      "source": [
        "print(\"Classes:\", class_names)\n",
        "print(\"Train/Val:\", len(X_tr), len(X_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdyT5LNKKOQz"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (200, 200)\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kisSUfmUKUnz"
      },
      "outputs": [],
      "source": [
        "def decode_and_resize(path, y):\n",
        "    x = tf.io.decode_image(tf.io.read_file(path), channels=1, expand_animations=False)\n",
        "    x = tf.image.convert_image_dtype(x, tf.float32)*2.0 - 1.0\n",
        "    x = tf.image.resize(x, IMAGE_SIZE, antialias=False)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irQQaanLKecJ"
      },
      "outputs": [],
      "source": [
        "augment = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.10),\n",
        "    layers.RandomTranslation(0.05, 0.05),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFfZXvQV8wHN"
      },
      "outputs": [],
      "source": [
        "def make_ds(X, y, augment_on=False, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle: ds = ds.shuffle(len(X), seed=42, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
        "    if augment_on:\n",
        "        ds = ds.map(lambda x, y: (augment(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
        "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gJAFgDk82uI"
      },
      "outputs": [],
      "source": [
        "train_ds = make_ds(X_tr,  y_tr,  augment_on=True,  shuffle=True)\n",
        "val_ds   = make_ds(X_val, y_val, augment_on=False, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2xXZjVK-Afv"
      },
      "outputs": [],
      "source": [
        "counts  = np.bincount(y_tr, minlength=len(class_names))\n",
        "weights = {i: (len(y_tr) / (len(class_names) * counts[i])) for i in range(len(class_names))}\n",
        "weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwDt19xb-EIz"
      },
      "outputs": [],
      "source": [
        "def build_baseline(num_classes):\n",
        "    inp = layers.Input((*IMAGE_SIZE, 3))\n",
        "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inp); x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x);  x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(128,3, padding='same', activation='relu')(x);  x = layers.GlobalAveragePooling2D()(x)\n",
        "    return keras.Model(inp, out, name=\"baseline_cnn\")\n",
        "\n",
        "model = build_baseline(len(class_names))\n",
        "model.compile(optimizer=keras.optimizers.Adam(2),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "cbs = [\n",
        "    keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
        "    keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1),\n",
        "]\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=6,\n",
        "                    class_weight=weights, callbacks=cbs, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob = model.predict(val_ds, verbose=0)\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "\n",
        "print(classification_report(y_val, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "plt.imshow(cm, cmap=\"Blues\"); plt.title(\"Confusion Matrix\"); plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
        "plt.xticks(range(len(class_names)), class_names, rotation=45); plt.yticks(range(len(class_names)), class_names)\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, cm[i, j], ha='center', va='center')\n",
        "plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "A1zULcwyNgZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (224, 224)\n",
        "NUM_CLASSES = len(class_names)\n",
        "\n",
        "cbs = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", patience=2, factor=0.5, min_lr=1e-6),\n",
        "    keras.callbacks.ModelCheckpoint(\"mobilenetv2_best.keras\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "]\n",
        "\n",
        "inputs = layers.Input(shape=(*IMAGE_SIZE, 3))\n",
        "\n",
        "x = layers.Rescaling(2.0, offset=-1.0)(inputs)\n",
        "\n",
        "base = MobileNetV2(include_top=False, weights=\"imagenet\", input_tensor=x)\n",
        "base.trainable = False\n",
        "\n",
        "y = layers.GlobalAveragePooling2D()(base.output)\n",
        "y = layers.Dropout(0.30)(y)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(y)\n",
        "tl_model = keras.Model(inputs, outputs, name=\"mobilenetv2_tl\")\n",
        "\n",
        "tl_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "print(\"Phase 1: training head only (base frozen)\")\n",
        "history_tl_1 = tl_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=8,\n",
        "    class_weight=globals().get(\"weights\", None),\n",
        "    callbacks=cbs,\n",
        "    verbose=1\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "g3uIibwoQk1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_hist(*hists):\n",
        "    acc, val_acc, loss, val_loss = [], [], [], []\n",
        "    for h in hists:\n",
        "        if h is None: continue\n",
        "        acc     += h.history.get('accuracy', [])\n",
        "        val_acc += h.history.get('val_accuracy', [])\n",
        "        loss    += h.history.get('loss', [])\n",
        "        val_loss+= h.history.get('val_loss', [])\n",
        "    return acc, val_acc, loss, val_loss\n",
        "\n",
        "h1 = globals().get('history_tl_1')\n",
        "h2 = globals().get('history_tl_2')\n",
        "acc, val_acc, loss, val_loss = merge_hist(h1, h2)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1); plt.plot(acc, label='train'); plt.plot(val_acc, label='val'); plt.title('Accuracy'); plt.legend()\n",
        "plt.subplot(1,2,2); plt.plot(loss, label='train'); plt.plot(val_loss, label='val'); plt.title('Loss'); plt.legend()\n",
        "plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "wjuaPrDagoxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_eval_ds():\n",
        "    if 'make_ds' in globals():\n",
        "        return make_ds(val_paths, y_val, augment=False, shuffle=False)\n",
        "    return val_ds\n",
        "\n",
        "eval_ds = make_eval_ds()\n",
        "\n",
        "y_prob = tl_model.predict(eval_ds, verbose=0)\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "y_true = y_val\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-9)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens',\n",
        "            xticklabels=class_names, yticklabels=class_names, vmin=0, vmax=1)\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix (Normalized)\")\n",
        "plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "1oONco54g8yV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}